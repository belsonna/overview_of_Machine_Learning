{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"logistic_regression_assignment.template.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"swNgYI-sgzcT","colab_type":"text"},"cell_type":"markdown","source":["# Logistic Regression Tutorial "]},{"metadata":{"id":"oyP-6JZMgzcV","colab_type":"code","colab":{}},"cell_type":"code","source":["## Do **not** change this cell, and do **not** import\n","## any other modules anywhere in the notebook.\n","import numpy as np\n","import numpy.random as rn\n","from scipy import optimize, stats\n","import scipy.linalg as linalg\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"D2aWcbcKgzca","colab_type":"text"},"cell_type":"markdown","source":["In this tutorial we're going to cover the basics behind logistic regression. For simplicity we will only consider the binary classification case, in which target variables are $y \\in \\{0,1\\}$. \n","\n","In logistic regression, the probability of a data point $\\boldsymbol x$ being of class 1 is given by\n","\n","$$p(y = 1 | \\boldsymbol x, \\boldsymbol\\theta) = \\sigma (\\boldsymbol x^\\top \\boldsymbol\\theta) ~ ,$$\n","\n","where $\\sigma(z) = 1/(1+\\exp(-z))$ is the _sigmoid_ function.\n","\n","Combining this with a Bernoulli likelihood and summing over all datapoints $\\{\\boldsymbol x_i, y_i\\}_{i=1}^N$ we end up with a negative log-likelihood function that looks like this:\n","\n","$$-\\log p(\\boldsymbol y|\\boldsymbol X, \\boldsymbol\\theta) = -\\sum_i\\left(y_i \\log \\sigma(\\boldsymbol x_i^\\top \\boldsymbol\\theta) + (1 - y_i) \\log ( 1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol\\theta))\\right)$$\n","\n","You will see this expression in many other classification problems, especially in deep learning, where it's known as the _cross-entropy loss_.\n","\n","Your goal in this tutorial is to learn how to perform inference over the parameters $\\boldsymbol\\theta$ in logistic regression, including point estimates $\\boldsymbol\\theta_{\\mathrm{ML}}$ and $\\boldsymbol\\theta_{\\mathrm{MAP}}$ and approximations to the posterior $p(\\boldsymbol\\theta | \\boldsymbol X, \\boldsymbol y)$.\n","\n","Let's do it."]},{"metadata":{"collapsed":true,"id":"WGvIChExgzcc","colab_type":"text"},"cell_type":"markdown","source":["## Maximum likelihood estimate"]},{"metadata":{"id":"X3CWmbGIgzcd","colab_type":"text"},"cell_type":"markdown","source":["Let's start easy. First, let's generate a toy 1D binary dataset with two paramaters:\n","\n","* A **jitter** parameter that controls how noisy the data are; and\n","* An **offset** parameter that controls the separation between the two classes."]},{"metadata":{"id":"9Py63ELdgzce","colab_type":"code","colab":{}},"cell_type":"code","source":["# Data generation parameters\n","N = 50\n","D = 2\n","jitter = 0.7\n","offset = 1.2\n","\n","# Generate the data\n","x = np.vstack([rn.normal(0, jitter, (N//2,1)), rn.normal(offset, jitter, (N//2,1))])\n","y = np.vstack([np.zeros((N//2, 1)), np.ones((N//2, 1))])\n","x_test = np.linspace(-2, offset + 2).reshape(-1,1)\n","\n","# Make the augmented data matrix by adding a column of ones\n","x = np.hstack([np.ones((N,1)), x])\n","x_test = np.hstack([np.ones((N,1)), x_test])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8CO0lzDsgzci","colab_type":"text"},"cell_type":"markdown","source":["Now on to the regression. First, let's code up the logistic log-likelihood as a separate function. This will come in handy.\n","\n","**Task 1**\n","\n","* Write a function to calculate the log-likelihood of a dataset given a value of $\\boldsymbol\\theta$."]},{"metadata":{"id":"tbWp_oAqgzcj","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def log_likelihood(X, y, theta):\n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # theta: parameters (D x 1)\n","    # returns: log likelihood, scalar\n","    \n","    L = - np.inf ## <-- EDIT THIS LINE\n","    \n","    return L"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9DrDFU8xgzco","colab_type":"text"},"cell_type":"markdown","source":["Now it's time to optimize it to fit the maximum likelihood parameter,\n","\n","$$\\boldsymbol\\theta_{\\mathrm{ML}} = \\mathrm{arg}_{\\boldsymbol\\theta} \\max p(\\boldsymbol y | \\boldsymbol X, \\boldsymbol\\theta)$$\n","\n","For linear regression, the likelihood function had a closed-form minimum, which made our lives easy. Alas, that is not the case for logistic regression. We will have to resort to _numerical optimization_.\n","\n","In the lectures you saw how to derive the gradient and all that jazz. For this tutorial you can do it that way, or any other way you want. The optimization is convex, so this should be easy peasy.\n","\n","**Task 2**\n","\n","* Write a function to optimize the log-likelihood function you've written above an obtain $\\boldsymbol\\theta_{\\mathrm{ML}}$. Use any optimizer of your choice."]},{"metadata":{"id":"iPBQ4Wc3gzcp","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def max_lik_estimate(X, y):\n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # returns: maximum likelihood parameters (D x 1)\n","    \n","    N, D = X.shape\n","    theta_ml = np.zeros((D,1)) ## <-- EDIT THIS LINE\n","    return theta_ml"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0juCl2Pfgzcs","colab_type":"text"},"cell_type":"markdown","source":["**Task 3**\n","\n","* Write a predict function to evaluate your estimate."]},{"metadata":{"id":"bV7Lrahdgzct","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def predict(X, theta):\n","    # Xtest: K x D matrix of test inputs\n","    # theta: D x 1 vector of parameters\n","    # returns: prediction of f(Xtest); K x 1 vector\n","    \n","    prediction = 0 ## <-- EDIT THIS LINE\n","    \n","    return prediction"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4_g8xSTZgzcy","colab_type":"text"},"cell_type":"markdown","source":["With this we're in a good position to fit a logistic regression to our toy dataset and start visualising the results. Have a go.\n","\n","1. Use the function you wrote above to estimate $\\boldsymbol\\theta_{\\mathrm{ML}}$ on the toy dataset.\n","2. Visualize the results, including:\n","    1. The data $x$ and target labels $y$. \n","    2. The labels predicted by the model.\n","    3. The probability assigned by the model, $\\sigma(x\\theta)$ as a function of $x$."]},{"metadata":{"id":"AbJ00J1ggzcy","colab_type":"code","colab":{}},"cell_type":"code","source":["## ADD CODE HERE\n","# Fit and plot the logistic regression\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"txKVHV73gzc5","colab_type":"text"},"cell_type":"markdown","source":["There you go! That should be a nice and easy fit. There are a few things we can start playing with at this point:\n","\n","* Evaluate the performance of your model: plot the decision boundary, likelihood and accuracy on held-out test sets, etc.\n","* Write a gradient-based and a non-gradient-based optimizer. Do they arrive at the same result? Which one takes longer? Which one evaluates the likelihood function more times?\n","\n","(Warning: if the plot looks odd and you get several warnings, it may be that the data is linearly separable and the sigmoid is saturating, leading to `np.log(0)` numerical problems. Add more noise and retry.)"]},{"metadata":{"id":"Mxy03xGygzc7","colab_type":"text"},"cell_type":"markdown","source":["## Bayesian logistic regression"]},{"metadata":{"id":"4Q1pMNrygzc9","colab_type":"text"},"cell_type":"markdown","source":["### MAP estimate"]},{"metadata":{"id":"Gmk5FeDHgzc-","colab_type":"text"},"cell_type":"markdown","source":["Now let's move to Bayesian inference on the parameters $\\boldsymbol\\theta$. Let's put a prior on them. Because that's what we do. We put priors on things.\n","\n","More specifically, let's use a Gaussian prior parametrized by a mean $\\boldsymbol m$ and a variance $\\boldsymbol S$:\n","\n","$$\\boldsymbol\\theta \\sim  \\mathcal{N}(\\boldsymbol m, \\boldsymbol S)$$\n","\n","Given that $\\boldsymbol\\theta_{\\mathrm{ML}}$ had no analytical solution, it should really come as no surprise that $\\boldsymbol\\theta_{\\mathrm{MAP}}$ doesn't either. That should be no problem for a machine learning expert like you:\n","\n","**Task 4**\n","\n","1. Write down the equation for the full unnormalized posterior $p(\\boldsymbol\\theta | \\boldsymbol X, \\boldsymbol y) \\propto p(\\boldsymbol y | \\boldsymbol\\theta, \\boldsymbol X) p(\\boldsymbol\\theta)$.\n","2. Write a separate function for it, as we did with the log-likelihood above.\n","3. Optimize it to find $\\boldsymbol\\theta_{\\mathrm{MAP}}$ and use it to make predictions."]},{"metadata":{"id":"csUBiB72gzdA","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def map_estimate(X, y, m, S):\n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # m: D x 1 prior mean of parameters\n","    # S: D x D prior covariance of parameters\n","    # returns: maximum a posteriori parameters (D x 1)\n","    \n","    N, D = X.shape\n","    theta_map = np.zeros((D,1)) ## <-- EDIT THIS LINE\n","    return theta_map"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6bDA3HdrgzdD","colab_type":"text"},"cell_type":"markdown","source":["Now you can perform a similar model evaluation as you did before. How does your prior influence the MAP estimate and the model's performance?"]},{"metadata":{"id":"UBa0HOX8gzdE","colab_type":"code","colab":{}},"cell_type":"code","source":["## ADD CODE HERE\n","# Fit and plot the MAP logistic regression estimate\n","\n","m = np.zeros((D, 1))\n","S = 5*np.eye(D)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dyegDKvvgzdI","colab_type":"text"},"cell_type":"markdown","source":["### The Laplace approximation"]},{"metadata":{"id":"2N7XBcNXgzdJ","colab_type":"text"},"cell_type":"markdown","source":["As we have hinted above, in logistic regression the posterior distribution over $\\boldsymbol\\theta$ doesn't have an analytical solution. This is the first example in the course of _approximate Bayesian inference_: The exact posterior is analytically intractable so that we have to approximate it using one of various techniques. The one we'll use in this part of the tutorial is called the **Laplace approximation**.\n","\n","In brief, **the Laplace approximation is a Gaussian centered at the peak of the pdf of interest with the same curvature**. Let's make this a bit more rigorous below.\n","\n","Let's say we have a probability distribution $p(\\boldsymbol z)$ we want to approximate. The distribution $p(\\boldsymbol z)$ is of the form\n","\n","$$p(\\boldsymbol z) = \\frac{1}{Z} \\tilde{p}(\\boldsymbol z) ~ ,$$\n","\n","where $\\tilde{p}(\\boldsymbol z)$ is an unnormalized distribution that we can evaluate easily, but $Z$ is unknown. Formally, the Laplace approximation results from a second-order Taylor expansion of $\\log \\tilde{p}(\\boldsymbol z)$ around $\\boldsymbol  z_0$:\n","\n","$$\\log \\tilde{p}(\\boldsymbol z) \\approx \\log \\tilde{p}(\\boldsymbol z_0) + \\frac{d}{d\\boldsymbol z}\\log \\tilde{p}(\\boldsymbol z)\\Big|_{\\boldsymbol z=\\boldsymbol z_0}(\\boldsymbol z -\\boldsymbol  z_0) + \\frac{1}{2}(\\boldsymbol z-\\boldsymbol z_0)^\\top\\frac{d^2}{d\\boldsymbol z^2} \\log \\tilde{p}(\\boldsymbol z)\\Big|_{\\boldsymbol z=\\boldsymbol z_0}(\\boldsymbol z-\\boldsymbol z_0)$$\n","\n","Now let's evaluate this expression at the mode of $p(\\boldsymbol z)$ &ndash; which is the same as the mode of $\\tilde{p}(\\boldsymbol z)$. We define the mode $\\boldsymbol z^*$ such that\n","\n","$$\\frac{d}{d\\boldsymbol z} \\tilde{p}(\\boldsymbol z) \\Big|_{\\boldsymbol z = \\boldsymbol z^*} = \\boldsymbol 0 ~ .$$\n","\n","At this point, the $\\mathcal{O}(\\boldsymbol z)$ term of the expansion vanishes and we are left with\n","\n","$$\\log \\tilde{p}(\\boldsymbol z) \\approx \\log \\tilde{p}(\\boldsymbol z^*) - \\frac{1}{2}(\\boldsymbol z-\\boldsymbol z^*)^\\top\\boldsymbol A(\\boldsymbol z-\\boldsymbol z^*)$$\n","\n","Or, equivalently,\n","\n","$$\\tilde{p}(\\boldsymbol z) \\approx \\tilde{p}(\\boldsymbol z^*) \\exp\\big(-\\tfrac{1}{2}(\\boldsymbol z - \\boldsymbol z^*)^\\top\\boldsymbol A(\\boldsymbol z - \\boldsymbol z^*)\\big) ~ ,$$\n","\n","where\n","\n","$$\\boldsymbol A = - \\frac{d^2}{d\\boldsymbol z^2} \\log \\tilde{p}(\\boldsymbol z)\\Big|_{\\boldsymbol z=\\boldsymbol z^*} ~ .$$\n","\n","And now this distribution we know how to normalize, because it's one of those Gaussians we know and love. By inspection, we can identify the mean and the covariance, and write down the Laplace approximation of $p(\\boldsymbol z)$ as\n","\n","$$q(\\boldsymbol z) = \\mathcal{N}(\\boldsymbol z | \\boldsymbol z^*, \\boldsymbol A^{-1})$$"]},{"metadata":{"id":"PRQx6WzBgzdL","colab_type":"text"},"cell_type":"markdown","source":["#### Example\n","As an example, let's use the unnormalized distribution $\\tilde{p}(z) = x e^{-x/2}$. When normalized properly, this is in fact the $\\chi^2$ distribution with $k=4$ degrees of freedom. Have a go yourself:\n","\n","1. Plot $p(z)$.\n","2. Take the first derivative of $\\tilde{p}(z)$ (or the first derivative of its log), and find its maximum $z^*$ analytically.\n","3. In the same plot, draw a vertical line at $z = z^*$ to verify you got the right answer.\n","4. Take the second derivative of $\\log \\tilde{p}(z)$ and evaluate it at $z^*$.\n","5. Plot the corresponding Gaussian $q(z)$ and verify the approximation looks reasonable.\n","\n","**Task 5**\n","\n","* Write a function that evaluates the Laplace approximation $q(z)$."]},{"metadata":{"id":"uXihhdpGgzdL","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def laplace_q(z):\n","    # z: double array of size (T,)\n","    # returns: array with Laplace approximation q evaluated\n","    #          at all points in z\n","\n","    q = 0*z\n","    return q"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rsNEBnmNgzdO","colab_type":"code","colab":{}},"cell_type":"code","source":["## ADD CODE HERE\n","# Find the Laplace approximation of x*exp(-x/2) with pen and paper and then plot it.\n","z = np.linspace(0,10)\n","p = stats.chi2.pdf(z, 4)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XTTAw61FgzdU","colab_type":"text"},"cell_type":"markdown","source":["### Bayesian logistic regression (for real this time)"]},{"metadata":{"id":"d4cs4w3NgzdW","colab_type":"text"},"cell_type":"markdown","source":["Now we have obtained the mode (peak) of the posterior through the MAP estimate above, it's time to go all the way and calculate the posterior over $\\boldsymbol\\theta$. However, as we mentioned above the posterior doesn't have an analytical form, so we'll use &ndash; you guessed it &ndash; the Laplace transform.\n","\n","**Task 6**\n","\n","* Write a function, based on your previous code, that will calculate the Laplace approximation $q(\\boldsymbol\\theta)$ of the true posterior $p(\\boldsymbol\\theta | \\boldsymbol X, \\boldsymbol y)$ and return the mean and variance of $q$.\n","\n","To visualize the behavior and the diversity of $q$, draw a number $j = 1, ..., J$ of samples $\\boldsymbol\\theta_j \\sim q(\\boldsymbol\\theta)$. For each sample, plot its predicted class probabilities $\\sigma(x \\boldsymbol\\theta_j)$.\n","\n","_Hint_: the extension of the Laplace approximation to multivariate distributions is straightforward, and in this case the variance of the Gaussian is the Hessian of the negative log likelihood $\\boldsymbol A = - \\nabla_\\theta \\nabla_\\theta \\log p(\\boldsymbol\\theta | \\boldsymbol X, \\boldsymbol y)$."]},{"metadata":{"id":"B2imRCu3gzda","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def get_posterior(X, y, m, S):\n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # m: D x 1 prior mean of parameters\n","    # S: D x D prior covariance of parameters\n","    # returns: maximum a posteriori parameters (D x 1)\n","    #          covariance of Laplace approximation (D x D)\n","\n","    mu_post = np.zeros((D, 1)) ## <-- EDIT THESE LINES\n","    S_post  = np.eye(D)\n","    \n","    return mu_post, S_post"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LPWJxWWkgzdf","colab_type":"code","colab":{}},"cell_type":"code","source":["## ADD CODE HERE\n","# Calculate the Laplace approximation of the posterior for theta, \n","# draw a few samples and plot the corresponding likelihood functions \n","# for each one.\n","m = np.zeros((D, 1))\n","S = 5*np.eye(D)\n","nb_samples = 5\n","\n","theta_map, S_post = get_posterior(x, y, m, S)\n","plt.scatter(x[:,1], y)\n","for i in range(nb_samples):\n","    plt.plot(0, 0) ## <--EDIT THIS LINE\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"7isyYHFfgzdo","colab_type":"text"},"cell_type":"markdown","source":["## Comparing posterior approximations"]},{"metadata":{"id":"guBN_J5qgzdr","colab_type":"text"},"cell_type":"markdown","source":["The Laplace approximation is part of a family of methods known as _deterministic approximate inference_. In addition, there's another set of methods known as _stochastic approximate inference_ which, as you can guess includes most of the sampling techniques you have studied.\n","\n","You must be an expert in sampling by now. Let's actually go and check whether this Laplace approximation we just made is legit.\n","\n","* What sampling methods do you know to sample from an unnormalized distribution?\n","\n","For example, let's try the Metropolis algorithm.\n","\n","1. Write a proposal function to move in $\\boldsymbol\\theta$-space.\n","2. Write a function to accept or reject new proposals based on the Metropolis criterion.\n","3. Write a loop and run the Markov chain for a few thousand iterations.\n","4. Check that the sampling worked: did the Markov chain mix properly? What's the acceptance rate? How does it depend on the proposal function?\n","\n","**Task 7**\n","\n","* Write a function to sample from the true posterior $p(\\boldsymbol\\theta | \\boldsymbol X, \\boldsymbol y)$."]},{"metadata":{"id":"9LnTPS__gzds","colab_type":"code","colab":{}},"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def posterior_sample(X, y, m, S, nb_iter):\n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # m: D x 1 prior mean of parameters\n","    # S: D x D prior covariance of parameters\n","    # returns: nb_iter x D matrix of posterior samples\n","    \n","    D = X.shape[1]\n","    samples = np.zeros((nb_iter, D))\n","    \n","    return samples"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H-GqbgQdgzdw","colab_type":"text"},"cell_type":"markdown","source":["Finally, let's plot the results and see if both inference methods arrive at roughly the same posterior.\n","\n","In the same axis, plot\n","\n","* The histogram pdf of the MCMC samples (you may want to look at the `density` option in `plt.hist`); and\n","* The Laplace posterior.\n","\n","Make one plot for the intercept ($\\theta_0$) and one for the slope ($\\theta_1$). What do they look like? Do they match? What kinds of posteriors do you think the Laplace approximation will be good or bad at approximating?"]},{"metadata":{"id":"SfSL2wx8gzdx","colab_type":"code","colab":{}},"cell_type":"code","source":["## ADD CODE HERE\n","# Plot a histogram of the MCMC posterior samples and the\n","# analytical expression for the Laplace posterior. If\n","# everything went well, the peaks should coincide and\n","# their widths should be comparable.\n","\n","nb_iter = 10000\n","samples = posterior_sample(x, y, m, S, nb_iter)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kVUVjyHagzd1","colab_type":"text"},"cell_type":"markdown","source":["Et violà! Now you're an expert in logistic regression. (Wait, I think that's a big violin. I meant to say: et voilà!)\n","\n","Now we can visualize the posterior we can play around with the data and the inference parameters:\n","\n","* Play around with the data generation process. What happens as you increase/decrease $N$ and the jitter parameter?\n","* What does the joint posterior look like? Make a visualization of the MCMC and Laplace approximations in the $(\\theta_0, \\theta_1)$ plane.\n","* What happens if the model is misspecified? Take out the intercept term in the model (i.e., remove the column of ones in $\\boldsymbol X$), but set the `offset` in the data generation process to non-zero. What happens to the posterior and its Laplace approximation?\n"]}]}